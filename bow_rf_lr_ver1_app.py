# -*- coding: utf-8 -*-
"""BoW_rf_lr_ver1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PSizNiaDC788p29CLJv_dtuHWvWsdljF
"""
#

import pandas as pd
import numpy as np
from numpy import random
import matplotlib.pyplot as plt
import os 

import re

import string
from string import punctuation

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
import sklearn.metrics as metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import nltk
from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag, word_tokenize

import gensim
from gensim.parsing.preprocessing import remove_stopwords

from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.models import Model, Sequential, load_model
from keras.layers import Input, Embedding, LSTM, Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping

from scikeras.wrappers import KerasClassifier

import pickle
from pickle import dump
from pickle import load

import warnings
warnings.filterwarnings('ignore')

nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("averaged_perceptron_tagger")
nltk.download('omw-1.4')

# pip install scikeras

stops = set(stopwords.words('english'))
#print(stops)

from google.colab import data_table
data_table.enable_dataframe_formatter

from google.colab import drive

drive.mount('/content/drive')

path = '/content/drive/MyDrive/train.csv'

df_raw = pd.read_csv(path)
#df_raw = pd.read_csv('train.csv')

print('shape of df  = ', df_raw.shape)
print()
df_raw.head(3)

# how to import csv - version2
# from google.colab import files
# uploaded = files.upload()

# import io  # StringIO
# df_raw = pd.read_csv(io.BytesIO(uploaded['train.csv']))

"""# EDA"""

df_raw.isnull().sum()

print('Value counts:\n', df_raw['is_duplicate'].value_counts())
print('\nPercent of labels value:\n', df_raw['is_duplicate'].value_counts()/df_raw['is_duplicate'].count()*100)
df_raw['is_duplicate'].value_counts().plot(kind = 'bar')
plt.title('\nDistribution of train labels', color = 'purple', size = 12)
plt.show()

# repeated questions
qid = pd.Series(df_raw['qid1'].tolist() + df_raw['qid2'].tolist())
print('Unique questions = ', len(np.unique(qid)))
rep = qid.value_counts() > 1
print('Repeted questions = ', len(rep[rep]))
# reapeted ques hist
plt.hist(qid.value_counts().values, bins = 180)
plt.yscale('log')
plt.show()

#  Some questions are repeated almost 160 times

df_raw_quest = df_raw[['question1', 'question2']]
df_raw_quest.head(3)

# analysis of features
q1_len = df_raw_quest['question1'].str.len()
sns.displot(q1_len)
print('max caracters = ', q1_len.max(), '\nmin caracters = ', q1_len.min(), '\naverage num of caracters = ', int(q1_len.mean()))

df_raw_quest.loc[(q1_len == 1)| (q1_len == 623)]

# analysis of features
q2_len = df_raw_quest['question2'].str.len()
sns.displot(q2_len)
print('max caracters = ', q2_len.max(), '\nmin caracters = ', q2_len.min(), '\naverage num of caracters = ', int(q2_len.mean()))

df_raw_quest.loc[(q2_len == 1) | (q2_len == 1169)]

"""# Preprocessing"""

df_raw = df_raw.fillna('empty') 
df_raw.isnull().sum()

stemmer =  PorterStemmer()
lemmer = WordNetLemmatizer()

# example
print(stemmer.stem('halves'))
print(lemmer.lemmatize('halves'))

print(stemmer.stem('children'))
print(lemmer.lemmatize('children'))

# randomly selected samples from df for the new df to perform operations

df = df_raw.sample(15000)
dfq = df[['question1', 'question2']]
print(df.shape)
dfq.head(3)

def clean_sentence(sentence, stopwords = False, lemmatize = False, stem = False):
    #sentence = []
    sentence = sentence.lower().strip()
    sentence = re.sub(r'[^a-z0-9\s]', '', sentence)  # removing all caracters that are not alpha numeric
    #sentence = re.sub(r'\s{2,}', ' ', sentence
    # remove punctuation
    sentence = sentence.translate(str.maketrans('', '', string.punctuation))
    
    if stopwords:
        sentence = remove_stopwords(sentence)  # stopwords are adding noises, and some algoritham can not handle 
        
    if lemmatize:
        sent_lemma = ''
        for word, tag in pos_tag(word_tokenize(sentence)):
            wntag = tag[0].lower()
            wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
            sent_lemma += ' ' + lemmer.lemmatize(word, wntag) if wntag else word
        sentence = sent_lemma

    if stem:
        sent_stemmed = ''
        for word in sentence.split():
            sent_stemmed += ' '+ stemmer.stem(word)
        sentence = sent_stemmed
   
    return sentence

def get_cleaned_senteces(df, stopwords = False, lemmatize = False, stem = False):
    sents1 = dfq[['question1']]  
    sents2 = dfq[['question2']]
    cleaned_sentences1 = []
    cleaned_sentences2 = []
    
    for index, row in df.iterrows():
        # print (index, row)
        cleaned1 = clean_sentence(row['question1'], stopwords, lemmatize, stem) 
        cleaned2 = clean_sentence(row['question2'], stopwords, lemmatize, stem)

        cleaned_sentences1.append(cleaned1)
        cleaned_sentences2.append(cleaned2)
    return cleaned_sentences1, cleaned_sentences2

def to_df(q1cleaned, q2cleaned, df):
    '''cleaned sentences to df'''
    X_temp1 = pd.DataFrame()
    X_temp2 = pd.DataFrame()
    X_temp1['q1'] = pd.DataFrame(q1cleaned, index = df.index)
    X_temp2['q2'] = pd.DataFrame(q2cleaned, index = df.index)
    X_temp = pd.concat([X_temp1, X_temp2], axis = 1)
    X_temp['is_duplicate'] = df['is_duplicate']
    return X_temp

# sw = stopwords, l = lemmatize, s = stem

# With stopwords, without lemma and stem
q1_with_sw, q2_with_sw = get_cleaned_senteces(df, stopwords = False, lemmatize = False, stem = False)
assert len(q1_with_sw) == len(q2_with_sw)
X_sw = to_df(q1_with_sw, q2_with_sw, df)
X_sw.shape

# With stopwords, without stem and lemmatized
q1_with_sw_l, q2_with_sw_l = get_cleaned_senteces(df, stopwords = False, lemmatize = True, stem = False)
assert len(q1_with_sw_l) == len(q2_with_sw_l)
X_sw_l = to_df(q1_with_sw_l, q2_with_sw_l, df)
X_sw_l.shape

# With stopwords, stemmed and without lemma
q1_with_sw_s, q2_with_sw_s = get_cleaned_senteces(df, stopwords = False, lemmatize = False, stem = True)
assert len(q1_with_sw_s) == len(q2_with_sw_s)
X_sw_s = to_df(q1_with_sw_s, q2_with_sw_s, df)
X_sw_s.shape

# Without stopwords, lemma and stem
q1_without_sw, q2_without_sw = get_cleaned_senteces(df, stopwords = True, lemmatize = False, stem = False)
assert len(q1_without_sw) == len(q1_without_sw)
X_without_sw = to_df(q1_without_sw, q2_without_sw, df)
X_without_sw.shape

# Without stopwords and stem, lemmatized
q1_without_sw_l, q2_without_sw_l = get_cleaned_senteces(df, stopwords = True, lemmatize = True, stem = False)
assert len(q1_without_sw_l) == len(q1_without_sw_l)
X_without_sw_l = to_df(q1_without_sw_l, q2_without_sw_l, df)
X_without_sw_l.shape

# Without stopwords and lemma, stemmed
q1_without_sw_s, q2_without_sw_s = get_cleaned_senteces(df, stopwords = True, lemmatize = False, stem = True)
assert len(q1_without_sw_s) == len(q1_without_sw_s)
X_without_sw_s = to_df(q1_without_sw_s, q2_without_sw_s, df)
X_without_sw_s.shape

df['is_duplicate'] = df_raw['is_duplicate']
print(df.shape)

display ('With stopwords, without lemma and stem', X_sw.iloc[[22]])
display ('With stopwords, lemmatized and without stem', X_sw_l.iloc[[22]])
display ('With stopwords, stemmed and without lemma', X_sw_s.iloc[[22]])
display ('Without stopwords, lemma and stem', X_without_sw.iloc[[22]])
display ('Without stopwords and stem, lemmatized', X_without_sw_l.iloc[[22]])
display ('Without stopwords and lemma, stemmed', X_without_sw_s.iloc[[22]])

"""### **BoW**"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.experimental import enable_halving_search_cv 
from sklearn.model_selection import ShuffleSplit, HalvingGridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

result_df = pd.DataFrame(index = ['accuracy', 'loss'])

def bow_train(what):
    '''CountVectorizer on train set and test set'''
    
    y = df['is_duplicate']
    cv = CountVectorizer(ngram_range = (1,2), min_df = 5, max_features = 2000, analyzer = 'word', strip_accents = 'unicode')
    X_trainBoW, X_testBoW, y_train, y_test = train_test_split(what.iloc[:, 0:-1], 
                                                                y, stratify = y, test_size = 0.2, random_state = 0)
    questions = list(X_trainBoW['q1']) + list(X_trainBoW['q2'])
    q1_arr, q2_arr = np.vsplit(cv.fit_transform(questions).toarray(), 2)
    temp_df1 = pd.DataFrame(q1_arr, index = X_trainBoW.index)
    temp_df2 = pd.DataFrame(q2_arr, index = X_trainBoW.index)
    X_BoW_temp_df = pd.concat([temp_df1, temp_df2], axis = 1)
    #X_BoW_temp_df['is_duplicate'] = df['is_duplicate']
    
    questions_test = list(X_testBoW['q1']) + list(X_testBoW['q2'])
    q1_arr_test, q2_arr_test = np.vsplit(cv.transform(questions_test).toarray(), 2)
    temp_df1_test = pd.DataFrame(q1_arr_test, index = X_testBoW.index)
    temp_df2_test = pd.DataFrame(q2_arr_test, index = X_testBoW.index)
    X_BoW_temp_df_test = pd.concat([temp_df1_test, temp_df2_test], axis = 1)
    #X_BoW_temp_df_test['is_duplicate'] = df['is_duplicate']
    
    #print('X_train shape = ', X_trainBoW.shape, type(X_trainBoW), '\ny_train shape = ', y_train.shape, type(y_train),
      #'\nX_test shape = ', X_testBoW.shape, type(X_testBoW), '\ny_test shape = ', y_test.shape, type(y_test))
    return X_BoW_temp_df, X_BoW_temp_df_test, y_train, y_test

def rf_clf(X_train, X_test, y_train, y_test):
    '''Parameter tuning for Random Forest Classifier and model fit'''
    
    splits = ShuffleSplit(n_splits = 1, test_size = .2, random_state = 0)
    rf_param_grid = {'n_estimators':[200, 500, 800], 'min_samples_split':[5, 15], 'max_depth': [70, 150, None]}
    rf_clf = RandomForestClassifier()
    
    rf_bow_search = HalvingGridSearchCV(rf_clf, rf_param_grid, cv = splits, factor = 2, scoring = 'accuracy', verbose = 0)
    rf_bow_search.fit(X_train, y_train)
    
    rf_bow_model = rf_bow_search.best_estimator_

    # Ovde e snimen regresorot kako model
    regressor = rf_bow_search.best_estimator_
    data = {"model": regressor, "get_cleaned_senteces": get_cleaned_senteces, "to_df":to_df,"clean_sentence":clean_sentence}

    with open('saved_steps1.pkl', 'wb') as file:
        pickle.dump(data, file)


    #__________________________________

    y_pred_rf_bow = rf_bow_model.predict(X_test)
    rf_loss_bow = metrics.log_loss(y_test, y_pred_rf_bow)
    rf_acc_bow = metrics.accuracy_score(y_test, y_pred_rf_bow)
    print (rf_bow_model)
    print('log loss', rf_loss_bow, '\nacc', rf_acc_bow)
    return rf_acc_bow, rf_loss_bow, y_pred_rf_bow



def lr_clf(X_train, X_test, y_train, y_test):
    '''Parameter tuning for Logistic Regression Classifier and model fit'''
    
    splits = ShuffleSplit(n_splits = 1, test_size = .2, random_state = 0)
    lr_param_grid = {'C':[100, 10, 1.0, 0.1, 0.01], 'solver':['sag', 'lbfgs']}
    lr_clf = LogisticRegression(penalty = 'l2', random_state = 0)
   
    lr_bow_search = HalvingGridSearchCV(lr_clf, lr_param_grid, cv = splits, factor = 2, scoring = 'accuracy', verbose = 0)
    lr_bow_search.fit(X_train, y_train)
    
    lr_bow_model = lr_bow_search.best_estimator_
    y_pred_lr_bow = lr_bow_model.predict(X_test)
    lr_loss_bow = metrics.log_loss(y_test, y_pred_lr_bow)
    lr_acc_bow = metrics.accuracy_score(y_test, y_pred_lr_bow)
    print (lr_bow_model)
    print('log loss', lr_loss_bow, '\nacc', lr_acc_bow)
    return lr_acc_bow, lr_loss_bow, y_pred_lr_bow

import xgboost as xgb

def xgb_clf(X_train, X_test, y_train, y_test):
    '''Parameter tuning for Logistic Regression Classifier and model fit'''
   
    splits = ShuffleSplit(n_splits = 1, test_size = .2, random_state = 0)
    xgb_param_grid = {'n_estimators': [50, 150, 300], 'max_depth': [4, 6, 8, 10]}
    xgb_clf = xgb.XGBClassifier(objective = 'binary:logistic', n_jobs = -1, random_state = 0)
   
    xgb_bow_search = HalvingGridSearchCV(xgb_clf, xgb_param_grid, cv = splits, factor = 2, scoring = 'accuracy', n_jobs=-1, verbose = 0)
    evaluation = [( X_train, y_train), ( X_test, y_test)]
    xgb_bow_search.fit(X_train, y_train, eval_set = evaluation, early_stopping_rounds = 10, verbose = False)
    
    xgb_bow_model = xgb_bow_search.best_params_
    xgb_bow_model_est = model.best_estimator_
    y_pred_xgb_bow = xgb_bow_model_est.predict_proba(X_test)
    xgb_loss_bow = log_loss(y_test, y_pred_xgb_bow)
    xgb_acc_bow = xgb_bow_model_est.best_score_
    print (xgb_bow_model)
    print('Train log loss ', log_loss(y_train, y_pred_xgb_bow), 'Test log loss ', xgb_loss_bow, '\nacc ', xgb_acc_bow)
    return xgb_acc_bow, xgb_loss_bow, y_pred_xgb_bow

"""#### **BoW with stopwords, without lemma and stem**"""

X_trainBoW_sw, X_testBoW_sw, y_train, y_test = bow_train(X_sw)

print(X_trainBoW_sw.shape)
X_trainBoW_sw.head(2)

print(X_testBoW_sw.shape)
X_testBoW_sw.head(2)

rf_acc_bow_sw, rf_loss_bow_sw, y_pred_bow_sw = rf_clf(X_trainBoW_sw, X_testBoW_sw, y_train, y_test)

lr_acc_bow_sw, lr_loss_bow_sw, y_pred_bow_sw_lr = lr_clf(X_trainBoW_sw, X_testBoW_sw, y_train, y_test)

xgb_acc_bow_sw, xgb_loss_bow_sw, y_pred_bow_sw_xgb = xgb_clf(X_trainBoW_sw_l.values, X_testBoW_sw_l.values, y_train.values, y_test.values)

result_df['rf_BoW_sw'] = rf_acc_bow_sw, rf_loss_bow_sw
result_df['lr_BoW_sw'] = lr_acc_bow_sw, lr_loss_bow_sw

result_df

# save data frame result_df
dump(result_df, open('result_df.pkl', 'wb'))

# load data frame result_df
result_df = load(open('result_df.pkl', 'rb'))
result_df

"""#### **BoW with stopwords, lemmatized without stem**"""

X_trainBoW_sw_l, X_testBoW_sw_l, y_train, y_test = bow_train(X_sw_l)

print(X_trainBoW_sw_l.shape)
X_trainBoW_sw_l.head(2)

print(X_testBoW_sw_l.shape)
X_testBoW_sw_l.head(2)

rf_acc_bow_sw_l, rf_loss_bow_sw_l, y_pred_bow_sw_l = rf_clf(X_trainBoW_sw_l, X_testBoW_sw_l, y_train, y_test)

lr_acc_bow_sw_l, lr_loss_bow_sw_l, y_pred_bow_sw_l_lr = lr_clf(X_trainBoW_sw_l, X_testBoW_sw_l, y_train, y_test)

result_df['rf_BoW_sw_l'] = rf_acc_bow_sw_l, rf_loss_bow_sw_l
result_df['lr_BoW_sw_l'] = lr_acc_bow_sw_l, lr_loss_bow_sw_l
result_df

"""#### **BoW with stopwords, stemmed without lema**"""

X_trainBoW_sw_s, X_testBoW_sw_s, y_train, y_test = bow_train(X_sw_s)

print(X_trainBoW_sw_s.shape)
X_trainBoW_sw_s.head(2)

print(X_testBoW_sw_s.shape)
X_testBoW_sw_s.head(2)

rf_acc_bow_sw_s, rf_loss_bow_sw_s, y_pred_bow_sw_s = rf_clf(X_trainBoW_sw_s, X_testBoW_sw_s, y_train, y_test)

lr_acc_bow_sw_s, lr_loss_bow_sw_s, y_pred_bow_sw_s_lr = lr_clf(X_trainBoW_sw_s, X_testBoW_sw_s, y_train, y_test)

result_df['rf_BoW_sw_s'] = rf_acc_bow_sw_s, rf_loss_bow_sw_s
result_df['lr_BoW_sw_s'] = lr_acc_bow_sw_s, lr_loss_bow_sw_s
result_df

"""#### **BoW without stopwords, stemm and lema**"""

X_trainBoW_wo_sw, X_testBoW_wo_sw, y_train, y_test = bow_train(X_without_sw)

print(X_trainBoW_wo_sw.shape)
X_trainBoW_wo_sw.head(2)

print(X_testBoW_wo_sw.shape)
X_testBoW_wo_sw.head(2)

rf_acc_bow_wo_sw, rf_loss_bow_wo_sw, y_pred_bow_wo_sw = rf_clf(X_trainBoW_wo_sw, X_testBoW_wo_sw, y_train, y_test)

lr_acc_bow_wo_sw, lr_loss_bow_wo_sw, y_pred_bow_wo_sw_lr = lr_clf(X_trainBoW_wo_sw, X_testBoW_wo_sw, y_train, y_test)

result_df['rf_BoW_wo_sw'] = rf_acc_bow_wo_sw, rf_loss_bow_wo_sw
result_df['lr_BoW_wo_sw'] = lr_acc_bow_wo_sw, lr_loss_bow_wo_sw
result_df

"""#### **BoW without stopwords and stemm, lemmatized**"""

X_trainBoW_wo_sw_l, X_testBoW_wo_sw_l, y_train, y_test = bow_train(X_without_sw_l)

print(X_trainBoW_wo_sw_l.shape)
X_trainBoW_wo_sw_l.head(2)

print(X_testBoW_wo_sw_l.shape)
X_testBoW_wo_sw_l.head(2)

rf_acc_bow_wo_sw_l, rf_loss_bow_wo_sw_l, y_pred_bow_wo_sw_l = rf_clf(X_trainBoW_wo_sw_l, X_testBoW_wo_sw_l, y_train, y_test)

lr_acc_bow_wo_sw_l, lr_loss_bow_wo_sw_l, y_pred_bow_wo_sw_l_lr = lr_clf(X_trainBoW_wo_sw_l, X_testBoW_wo_sw_l, y_train, y_test)

result_df['rf_BoW_wo_sw_l'] = rf_acc_bow_wo_sw_l, rf_loss_bow_wo_sw_l
result_df['lr_BoW_wo_sw_l'] = lr_acc_bow_wo_sw_l, lr_loss_bow_wo_sw_l
result_df

"""#### **BoW without stopwords and lemma, steemed**"""

X_trainBoW_wo_sw_s, X_testBoW_wo_sw_s, y_train, y_test = bow_train(X_without_sw_s)

print(X_trainBoW_wo_sw_s.shape)
X_trainBoW_wo_sw_s.head(2)

print(X_testBoW_wo_sw_s.shape)
X_testBoW_wo_sw_s.head(2)

rf_acc_bow_wo_sw_s, rf_loss_bow_wo_sw_s, y_pred_bow_wo_sw_s = rf_clf(X_trainBoW_wo_sw_s, X_testBoW_wo_sw_s, y_train, y_test)

lr_acc_bow_wo_sw_s, lr_loss_bow_wo_sw_s, y_pred_bow_wo_sw_s_lr = lr_clf(X_trainBoW_wo_sw_s, X_testBoW_wo_sw_s, y_train, y_test)

#result_df['rf_BoW_wo_sw_s'] = rf_acc_bow_wo_sw_s, rf_loss_bow_wo_sw_s
result_df['lr_BoW_wo_sw_s'] = lr_acc_bow_wo_sw_s, lr_loss_bow_wo_sw_s
result_df

dump(result_df, open('result_df.pkl', 'wb'))

result_df = load(open('result_df.pkl', 'rb'))
result_df



